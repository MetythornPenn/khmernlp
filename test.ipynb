{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'path_pythainlp_corpus' from 'khmernlp.corpus.core' (/Users/macbookair/code/projects/khmerASR/pythainlp/khmernlp/corpus/core.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mheapq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m heappush, heappop  \u001b[38;5;66;03m# for priority queue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from marisa_trie import Trie\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_WORD_DICT_TRIE\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# wordlist = [li.strip() for li in open('./khmer_dictionary.txt', 'r', encoding='utf-8')]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# trie = Trie(wordlist)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m trie \u001b[38;5;241m=\u001b[39m DEFAULT_WORD_DICT_TRIE\n",
      "File \u001b[0;32m~/code/projects/khmerASR/pythainlp/khmernlp/tokenize/__init__.py:21\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mTokenizers at different levels of linguistic analysis.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTHAI2FIT_TOKENIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_tokenize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_syllables, khmer_words\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrie\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trie\n\u001b[1;32m     24\u001b[0m DEFAULT_WORD_TOKENIZE_ENGINE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewmm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/code/projects/khmerASR/pythainlp/khmernlp/corpus/__init__.py:78\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Get local path of corpus catalog.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _CORPUS_DB_PATH\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     79\u001b[0m     download,\n\u001b[1;32m     80\u001b[0m     get_corpus,\n\u001b[1;32m     81\u001b[0m     get_corpus_as_is,\n\u001b[1;32m     82\u001b[0m     get_corpus_db,\n\u001b[1;32m     83\u001b[0m     get_corpus_db_detail,\n\u001b[1;32m     84\u001b[0m     get_corpus_default_db,\n\u001b[1;32m     85\u001b[0m     get_corpus_path,\n\u001b[1;32m     86\u001b[0m     get_path_folder_corpus,\n\u001b[1;32m     87\u001b[0m     path_pythainlp_corpus,\n\u001b[1;32m     88\u001b[0m     remove,\n\u001b[1;32m     89\u001b[0m )  \u001b[38;5;66;03m# these imports must come before other pythainlp.corpus.* imports\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m     khmer_words,\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'path_pythainlp_corpus' from 'khmernlp.corpus.core' (/Users/macbookair/code/projects/khmerASR/pythainlp/khmernlp/corpus/core.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from heapq import heappush, heappop  # for priority queue\n",
    "# from marisa_trie import Trie\n",
    "\n",
    "from khmernlp.tokenize import DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# wordlist = [li.strip() for li in open('./khmer_dictionary.txt', 'r', encoding='utf-8')]\n",
    "# trie = Trie(wordlist)\n",
    "\n",
    "trie = DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "# ช่วยตัดพวกภาษาอังกฤษ เป็นต้น\n",
    "pat_eng = re.compile(r'''(?x)\n",
    "[-a-zA-Z]+|   # english\n",
    "\\d[\\d,\\.]*|   # number\n",
    "[ \\t]+|       # space\n",
    "\\r?\\n         # newline\n",
    "''')\n",
    "\n",
    "\n",
    "\n",
    "def onecut(text):\n",
    "  words_at = defaultdict(list)  # main data structure\n",
    "\n",
    "  def serialize(p, p2):    # helper function\n",
    "    for w in words_at[p]:\n",
    "      p_ = p + len(w)\n",
    "      if p_== p2:\n",
    "        yield [w]\n",
    "      elif p_ < p2:\n",
    "        for path in serialize(p_, p2):\n",
    "          yield [w]+path\n",
    "\n",
    "  q = [0]       # min-heap queue\n",
    "  last_p = 0    # last position for yield\n",
    "  while q[0] < len(text):\n",
    "      p = heappop(q)\n",
    "\n",
    "      for w in trie.prefixes(text[p:]):\n",
    "          words_at[p].append(w)\n",
    "          if p+len(w) not in q:\n",
    "            heappush(q, p+len(w))\n",
    "\n",
    "      if len(q)==1:\n",
    "          for w in min(serialize(last_p, q[0]), key=len):\n",
    "            yield w\n",
    "          last_p = q[0]\n",
    "\n",
    "      # กรณี len(q) == 0  คือ ไม่มีใน dict\n",
    "      if len(q)==0:\n",
    "          m = pat_eng.match(text[p:])\n",
    "          if m: # อังกฤษ, เลข, ว่าง\n",
    "              i = p + m.span()[1]\n",
    "          else: # skip น้อยที่สุด ที่เป็นไปได้\n",
    "              for i in range(p, len(text)):\n",
    "                  ww = trie.prefixes(text[i:])\n",
    "                  m = pat_eng.match(text[i:])\n",
    "                  if ww or m:\n",
    "                      break\n",
    "              else:\n",
    "                  i = len(text)\n",
    "          w = text[p:i]\n",
    "          words_at[p].append(w)\n",
    "          yield w\n",
    "          last_p = i\n",
    "          heappush(q, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khmerasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
