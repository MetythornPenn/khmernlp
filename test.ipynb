{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\khmernlp\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for lst20 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/lst20\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "d:\\code\\projects\\khmernlp\\khmernlp\\dataset\\x.tart.gz does not exist. Make sure you insert a manual dir via `datasetts.load_dataset('lst20', data_dir=...)`. Manual download instructions:   You need to\n  1. Manually download `AIFORTHAI-LST20Corpus.tar.gz` from https://aiforthai.in.th/corpus.php (login required; website mostly in Thai)\n  2. Extract the .tar.gz; this will result in folder `LST20Corpus`\n  The <path/to/folder> can e.g. be `~/Downloads/LST20Corpus`.\n  lst20 can then be loaded using the following command `datasets.load_dataset(\"lst20\", data_dir=\"<path/to/folder>\")`.\n  )",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlst20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./khmernlp/dataset/x.tart.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\khmernlp\\lib\\site-packages\\datasets\\load.py:2609\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2608\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2609\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2618\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2619\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2620\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\khmernlp\\lib\\site-packages\\datasets\\builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m-> 1027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1028\u001b[0m         dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m   1029\u001b[0m         verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   1030\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m   1032\u001b[0m     )\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\khmernlp\\lib\\site-packages\\datasets\\builder.py:1789\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1789\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1790\u001b[0m         dl_manager,\n\u001b[0;32m   1791\u001b[0m         verification_mode,\n\u001b[0;32m   1792\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1795\u001b[0m     )\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\khmernlp\\lib\\site-packages\\datasets\\builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[0;32m   1099\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m-> 1100\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\lst20\\e1b2a921fb011578ab43ddbbf789f3c500d62cb2df8ae4ed4b60bae8e4c0d3ad\\lst20.py:121\u001b[0m, in \u001b[0;36mLst20._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# check if manual folder exists\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_dir):\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist. Make sure you insert a manual dir via `datasetts.load_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlst20\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, data_dir=...)`. Manual download instructions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# check number of .txt files\u001b[39;00m\n\u001b[0;32m    126\u001b[0m nb_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: d:\\code\\projects\\khmernlp\\khmernlp\\dataset\\x.tart.gz does not exist. Make sure you insert a manual dir via `datasetts.load_dataset('lst20', data_dir=...)`. Manual download instructions:   You need to\n  1. Manually download `AIFORTHAI-LST20Corpus.tar.gz` from https://aiforthai.in.th/corpus.php (login required; website mostly in Thai)\n  2. Extract the .tar.gz; this will result in folder `LST20Corpus`\n  The <path/to/folder> can e.g. be `~/Downloads/LST20Corpus`.\n  lst20 can then be loaded using the following command `datasets.load_dataset(\"lst20\", data_dir=\"<path/to/folder>\")`.\n  )"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lst20\", data_dir=\"./khmernlp/dataset/x.tart.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'path_pythainlp_corpus' from 'khmernlp.corpus.core' (/Users/macbookair/code/projects/khmerASR/pythainlp/khmernlp/corpus/core.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mheapq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m heappush, heappop  \u001b[38;5;66;03m# for priority queue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from marisa_trie import Trie\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_WORD_DICT_TRIE\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# wordlist = [li.strip() for li in open('./khmer_dictionary.txt', 'r', encoding='utf-8')]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# trie = Trie(wordlist)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m trie \u001b[38;5;241m=\u001b[39m DEFAULT_WORD_DICT_TRIE\n",
      "File \u001b[0;32m~/code/projects/khmerASR/pythainlp/khmernlp/tokenize/__init__.py:21\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mTokenizers at different levels of linguistic analysis.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTHAI2FIT_TOKENIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_tokenize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_syllables, khmer_words\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrie\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trie\n\u001b[1;32m     24\u001b[0m DEFAULT_WORD_TOKENIZE_ENGINE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewmm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/code/projects/khmerASR/pythainlp/khmernlp/corpus/__init__.py:78\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Get local path of corpus catalog.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _CORPUS_DB_PATH\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     79\u001b[0m     download,\n\u001b[1;32m     80\u001b[0m     get_corpus,\n\u001b[1;32m     81\u001b[0m     get_corpus_as_is,\n\u001b[1;32m     82\u001b[0m     get_corpus_db,\n\u001b[1;32m     83\u001b[0m     get_corpus_db_detail,\n\u001b[1;32m     84\u001b[0m     get_corpus_default_db,\n\u001b[1;32m     85\u001b[0m     get_corpus_path,\n\u001b[1;32m     86\u001b[0m     get_path_folder_corpus,\n\u001b[1;32m     87\u001b[0m     path_pythainlp_corpus,\n\u001b[1;32m     88\u001b[0m     remove,\n\u001b[1;32m     89\u001b[0m )  \u001b[38;5;66;03m# these imports must come before other pythainlp.corpus.* imports\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkhmernlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m     khmer_words,\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'path_pythainlp_corpus' from 'khmernlp.corpus.core' (/Users/macbookair/code/projects/khmerASR/pythainlp/khmernlp/corpus/core.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from heapq import heappush, heappop  # for priority queue\n",
    "# from marisa_trie import Trie\n",
    "\n",
    "from khmernlp.tokenize import DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# wordlist = [li.strip() for li in open('./khmer_dictionary.txt', 'r', encoding='utf-8')]\n",
    "# trie = Trie(wordlist)\n",
    "\n",
    "trie = DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "# ช่วยตัดพวกภาษาอังกฤษ เป็นต้น\n",
    "pat_eng = re.compile(r'''(?x)\n",
    "[-a-zA-Z]+|   # english\n",
    "\\d[\\d,\\.]*|   # number\n",
    "[ \\t]+|       # space\n",
    "\\r?\\n         # newline\n",
    "''')\n",
    "\n",
    "\n",
    "\n",
    "def onecut(text):\n",
    "  words_at = defaultdict(list)  # main data structure\n",
    "\n",
    "  def serialize(p, p2):    # helper function\n",
    "    for w in words_at[p]:\n",
    "      p_ = p + len(w)\n",
    "      if p_== p2:\n",
    "        yield [w]\n",
    "      elif p_ < p2:\n",
    "        for path in serialize(p_, p2):\n",
    "          yield [w]+path\n",
    "\n",
    "  q = [0]       # min-heap queue\n",
    "  last_p = 0    # last position for yield\n",
    "  while q[0] < len(text):\n",
    "      p = heappop(q)\n",
    "\n",
    "      for w in trie.prefixes(text[p:]):\n",
    "          words_at[p].append(w)\n",
    "          if p+len(w) not in q:\n",
    "            heappush(q, p+len(w))\n",
    "\n",
    "      if len(q)==1:\n",
    "          for w in min(serialize(last_p, q[0]), key=len):\n",
    "            yield w\n",
    "          last_p = q[0]\n",
    "\n",
    "      # กรณี len(q) == 0  คือ ไม่มีใน dict\n",
    "      if len(q)==0:\n",
    "          m = pat_eng.match(text[p:])\n",
    "          if m: # อังกฤษ, เลข, ว่าง\n",
    "              i = p + m.span()[1]\n",
    "          else: # skip น้อยที่สุด ที่เป็นไปได้\n",
    "              for i in range(p, len(text)):\n",
    "                  ww = trie.prefixes(text[i:])\n",
    "                  m = pat_eng.match(text[i:])\n",
    "                  if ww or m:\n",
    "                      break\n",
    "              else:\n",
    "                  i = len(text)\n",
    "          w = text[p:i]\n",
    "          words_at[p].append(w)\n",
    "          yield w\n",
    "          last_p = i\n",
    "          heappush(q, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khmerasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
