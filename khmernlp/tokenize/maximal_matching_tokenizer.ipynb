{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Matching Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from heapq import heappush, heappop  # for priority queue\n",
    "from marisa_trie import Trie\n",
    "\n",
    "# from khmernlp.tokenize import DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wordlist = [li.strip() for li in open('./khmer_words.txt', 'r', encoding='utf-8')]\n",
    "trie = Trie(wordlist)\n",
    "\n",
    "# trie = DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "# ช่วยตัดพวกภาษาอังกฤษ เป็นต้น\n",
    "pat_eng = re.compile(r'''(?x)\n",
    "[-a-zA-Z]+|   # english\n",
    "\\d[\\d,\\.]*|   # number\n",
    "[ \\t]+|       # space\n",
    "\\r?\\n         # newline\n",
    "'''\n",
    "' '\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def onecut(text):\n",
    "  words_at = defaultdict(list)  # main data structure\n",
    "\n",
    "  def serialize(p, p2):    # helper function\n",
    "    for w in words_at[p]:\n",
    "      p_ = p + len(w)\n",
    "      if p_== p2:\n",
    "        yield [w]\n",
    "      elif p_ < p2:\n",
    "        for path in serialize(p_, p2):\n",
    "          yield [w]+path\n",
    "\n",
    "  q = [0]       # min-heap queue\n",
    "  last_p = 0    # last position for yield\n",
    "  while q[0] < len(text):\n",
    "      p = heappop(q)\n",
    "\n",
    "      for w in trie.prefixes(text[p:]):\n",
    "          words_at[p].append(w)\n",
    "          if p+len(w) not in q:\n",
    "            heappush(q, p+len(w))\n",
    "\n",
    "      if len(q)==1:\n",
    "          for w in min(serialize(last_p, q[0]), key=len):\n",
    "            yield w\n",
    "          last_p = q[0]\n",
    "\n",
    "      if len(q)==0:\n",
    "          m = pat_eng.match(text[p:])\n",
    "          if m:\n",
    "              i = p + m.span()[1]\n",
    "          else:\n",
    "              for i in range(p, len(text)):\n",
    "                  ww = trie.prefixes(text[i:])\n",
    "                  m = pat_eng.match(text[i:])\n",
    "                  if ww or m:\n",
    "                      break\n",
    "              else:\n",
    "                  i = len(text)\n",
    "          w = text[p:i]\n",
    "          words_at[p].append(w)\n",
    "          yield w\n",
    "          last_p = i\n",
    "          heappush(q, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "ជារឿងគួរឱ្យរំភើប បច្ចេកវិទ្យាតាមដានទឹកជំនន់ច្នៃបង្កើតឡើងដោយក្រុមហ៊ុន ArrowDot មានមូលដ្ឋាននៅកម្ពុជា មានស្ថាបនិក\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare result Maximal matching tokenizer vs Khmercut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n', 'ជា', 'រឿង', 'គួរ', 'ឱ្យ', 'រំភើប', ' ', 'បច្ចេកវិទ្យា', 'តាមដាន', 'ទឹកជំនន់', 'ច្នៃ', 'បង្កើតឡើង', 'ដោយ', 'ក្រុមហ៊ុន', ' ', 'ArrowDot', ' ', 'មាន', 'មូលដ្ឋាន', 'នៅ', 'កម្ពុជា', ' ', 'មាន', 'ស្ថាបនិក', '\\n', '\\n', '\\n']\n",
      "ជា\n",
      "រឿង\n",
      "គួរ\n",
      "ឱ្យ\n",
      "រំភើប\n",
      "បច្ចេកវិទ្យា\n",
      "តាមដាន\n",
      "ទឹកជំនន់\n",
      "ច្នៃ\n",
      "បង្កើតឡើង\n",
      "ដោយ\n",
      "ក្រុមហ៊ុន\n",
      "ArrowDot\n",
      "មាន\n",
      "មូលដ្ឋាន\n",
      "នៅ\n",
      "កម្ពុជា\n",
      "មាន\n",
      "ស្ថាបនិក\n"
     ]
    }
   ],
   "source": [
    "print(list(onecut(text)))\n",
    "\n",
    "list_words = list(onecut(text))\n",
    "\n",
    "ignore_words = ['\\n', ' ', '  ', '\\u200b', '\\t', '\\r']\n",
    "\n",
    "for word in list_words:\n",
    "    if word not in ignore_words:\n",
    "        print(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n',\n",
       " 'ជា',\n",
       " 'រឿង',\n",
       " 'គួរឱ្យ',\n",
       " 'រំភើប',\n",
       " ' ',\n",
       " 'បច្ចេកវិទ្យា',\n",
       " 'តាមដាន',\n",
       " 'ទឹក',\n",
       " 'ជំនន់',\n",
       " 'ច្នៃ',\n",
       " 'បង្កើតឡើង',\n",
       " 'ដោយ',\n",
       " 'ក្រុមហ៊ុន',\n",
       " ' ',\n",
       " 'ArrowDot',\n",
       " ' ',\n",
       " 'មាន',\n",
       " 'មូលដ្ឋាន',\n",
       " 'នៅ',\n",
       " 'កម្ពុជា',\n",
       " ' ',\n",
       " 'មាន',\n",
       " 'ស្ថាបនិក',\n",
       " '\\n\\n\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "\n",
    "tokenize(text)\n",
    "\n",
    "# list_words_2 = list(tokenize(text))\n",
    "# list_words_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary \n",
    "Overall khmercut outperform maximal matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trie data structure.\n",
    "\n",
    "Designed to be used for tokenizer's dictionary, but can be for other purposes.\n",
    "\"\"\"\n",
    "from typing import Iterable, Iterator, List, Union\n",
    "\n",
    "\n",
    "class Trie(Iterable[str]):\n",
    "    class Node:\n",
    "        __slots__ = \"end\", \"children\"\n",
    "\n",
    "        def __init__(self):\n",
    "            self.end = False\n",
    "            self.children = {}\n",
    "\n",
    "    def __init__(self, words: Iterable[str]):\n",
    "        self.words = set(words)\n",
    "        self.root = Trie.Node()\n",
    "\n",
    "        for word in words:\n",
    "            self.add(word)\n",
    "\n",
    "    def add(self, word: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a word to the trie.\n",
    "        Spaces in front of and following the word will be removed.\n",
    "\n",
    "        :param str text: a word\n",
    "        \"\"\"\n",
    "        word = word.strip()\n",
    "        self.words.add(word)\n",
    "        cur = self.root\n",
    "        for ch in word:\n",
    "            child = cur.children.get(ch)\n",
    "            if not child:\n",
    "                child = Trie.Node()\n",
    "                cur.children[ch] = child\n",
    "            cur = child\n",
    "        cur.end = True\n",
    "\n",
    "    def remove(self, word: str) -> None:\n",
    "        \"\"\"\n",
    "        Remove a word from the trie.\n",
    "        If the word is not found, do nothing.\n",
    "\n",
    "        :param str text: a word\n",
    "        \"\"\"\n",
    "        # remove from set first\n",
    "        if word not in self.words:\n",
    "            return\n",
    "        self.words.remove(word)\n",
    "        # then remove from nodes\n",
    "        parent = self.root\n",
    "        data = []  # track path to leaf\n",
    "        for ch in word:\n",
    "            child = parent.children[ch]\n",
    "            data.append((parent, child, ch))\n",
    "            parent = child\n",
    "        # remove the last one\n",
    "        child.end = False\n",
    "        # prune up the tree\n",
    "        for parent, child, ch in reversed(data):\n",
    "            if child.end or child.children:\n",
    "                break\n",
    "            del parent.children[ch]  # remove from parent dict\n",
    "\n",
    "    def prefixes(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all possible words from first sequence of characters in a word.\n",
    "\n",
    "        :param str text: a word\n",
    "        :return: a list of possible words\n",
    "        :rtype: List[str]\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        cur = self.root\n",
    "        for i, ch in enumerate(text):\n",
    "            node = cur.children.get(ch)\n",
    "            if not node:\n",
    "                break\n",
    "            if node.end:\n",
    "                res.append(text[: i + 1])\n",
    "            cur = node\n",
    "        return res\n",
    "\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.words\n",
    "\n",
    "    def __iter__(self) -> Iterator[str]:\n",
    "        yield from self.words\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.words)\n",
    "\n",
    "\n",
    "\n",
    "def dict_trie(dict_source: Union[str, Iterable[str], Trie]) -> Trie:\n",
    "    \"\"\"\n",
    "    Create a dictionary trie from a file or an iterable.\n",
    "\n",
    "    :param str|Iterable[str]|pythainlp.util.Trie dict_source: a path to\n",
    "        dictionary file or a list of words or a pythainlp.util.Trie object\n",
    "    :return: a trie object\n",
    "    :rtype: pythainlp.util.Trie\n",
    "    \"\"\"\n",
    "    trie = Trie([])\n",
    "\n",
    "    if isinstance(dict_source, str) and len(dict_source) > 0:\n",
    "        # dict_source is a path to dictionary text file\n",
    "        with open(dict_source, \"r\", encoding=\"utf8\") as f:\n",
    "            _vocabs = f.read().splitlines()\n",
    "            trie = Trie(_vocabs)\n",
    "    elif isinstance(dict_source, Iterable) and not isinstance(\n",
    "        dict_source, str\n",
    "    ):\n",
    "        # Note: Since Trie and str are both Iterable,\n",
    "        # so the Iterable check should be here, at the very end,\n",
    "        # because it has less specificality\n",
    "        trie = Trie(dict_source)\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"Type of dict_source must be pythainlp.util.Trie, \"\n",
    "            \"or Iterable[str], or non-empty str (path to source file)\"\n",
    "        )\n",
    "\n",
    "    return trie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import FrozenSet, List, Union\n",
    "import warnings\n",
    "\n",
    "\n",
    "_KHMER_WORDS: FrozenSet[str] = frozenset()\n",
    "_KHMER_WORDS_FILENAME = \"khmer_words.txt\"\n",
    "\n",
    "\n",
    "def get_corpus(path: str, comments: bool = True) -> frozenset:\n",
    "    path = path.strip()\n",
    "    lines = []\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as fh:\n",
    "        lines = fh.read().splitlines()\n",
    "\n",
    "    if not comments:\n",
    "        # if the line has a '#' character, take only text before the first '#'\n",
    "        lines = [line.split(\"#\", 1)[0].strip() for line in lines]\n",
    "\n",
    "    return frozenset(filter(None, lines))\n",
    "\n",
    "def khmer_words() -> FrozenSet[str]:\n",
    "    \"\"\"\n",
    "\n",
    "    :return: :class:`frozenset` containing words in the Khmer language.\n",
    "    :rtype: :class:`frozenset`\n",
    "    \"\"\"\n",
    "    global _KHMER_WORDS\n",
    "    if not _KHMER_WORDS:\n",
    "        _KHMER_WORDS = get_corpus(_KHMER_WORDS_FILENAME)\n",
    "\n",
    "    return _KHMER_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Multi cut -- Thai word segmentation with maximum matching.\n",
    "Original codes from Korakot Chaovavanich.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Iterator, List\n",
    "\n",
    "\n",
    "DEFAULT_WORD_DICT_TRIE = Trie(khmer_words())\n",
    "# DEFAULT_WORD_DICT_TRIE = Trie(\"./khmer_words.txt\")\n",
    "\n",
    "\n",
    "\n",
    "class LatticeString(str):\n",
    "    \"\"\"String that keeps possible tokenizations\"\"\"\n",
    "\n",
    "    def __new__(cls, value, multi=None, in_dict=True):\n",
    "        return str.__new__(cls, value)\n",
    "\n",
    "    def __init__(self, value, multi=None, in_dict=True):\n",
    "        self.unique = True\n",
    "        if multi:\n",
    "            self.multi = list(multi)\n",
    "            if len(self.multi) > 1:\n",
    "                self.unique = False\n",
    "        else:\n",
    "            self.multi = [value]\n",
    "        self.in_dict = in_dict  # if in dictionary\n",
    "\n",
    "\n",
    "_RE_NONKHMER = r\"\"\"(?x)\n",
    "[-a-zA-Z]+|       # Latin characters\n",
    "\\d+([,\\.]\\d+)*|   # numbers\n",
    "[ \\t]+|           # spaces\n",
    "\\r?\\n             # newlines\n",
    "\"\"\"\n",
    "_PAT_NONKHMER = re.compile(_RE_NONKHMER)\n",
    "\n",
    "\n",
    "def _multicut(\n",
    "    text: str, custom_dict: Trie = DEFAULT_WORD_DICT_TRIE\n",
    ") -> Iterator[LatticeString]:\n",
    "    \"\"\"Return LatticeString\"\"\"\n",
    "    if not custom_dict:\n",
    "        custom_dict = DEFAULT_WORD_DICT_TRIE\n",
    "\n",
    "    len_text = len(text)\n",
    "    words_at = defaultdict(list)  # main data structure\n",
    "\n",
    "    def serialize(p, p2):  # helper function\n",
    "        for w in words_at[p]:\n",
    "            p_ = p + len(w)\n",
    "            if p_ == p2:\n",
    "                yield w\n",
    "            elif p_ < p2:\n",
    "                for path in serialize(p_, p2):\n",
    "                    yield w + \"/\" + path\n",
    "\n",
    "    q = {0}\n",
    "    last_p = 0  # last position for yield\n",
    "    while min(q) < len_text:\n",
    "        p = min(q)\n",
    "        q -= {p}  # q.pop, but for set\n",
    "\n",
    "        for w in custom_dict.prefixes(text[p:]):\n",
    "            words_at[p].append(w)\n",
    "            q.add(p + len(w))\n",
    "\n",
    "        len_q = len(q)\n",
    "\n",
    "        if len_q == 1:\n",
    "            q0 = min(q)\n",
    "            yield LatticeString(text[last_p:q0], serialize(last_p, q0))\n",
    "            last_p = q0\n",
    "        elif len_q == 0:  # len(q) == 0  means not found in dictionary\n",
    "            m = _PAT_NONKHMER.match(text[p:])\n",
    "            if m:  # non-Thai token\n",
    "                i = p + m.span()[1]\n",
    "            else:  # non-Thai token, find minimum skip\n",
    "                for i in range(p, len_text):\n",
    "                    ww = custom_dict.prefixes(text[i:])\n",
    "                    m = _PAT_NONKHMER.match(text[i:])\n",
    "                    if ww or m:\n",
    "                        break\n",
    "                else:\n",
    "                    i = len_text\n",
    "            w = text[p:i]\n",
    "            words_at[p].append(w)\n",
    "            yield LatticeString(w, in_dict=False)\n",
    "            last_p = i\n",
    "            q.add(i)\n",
    "\n",
    "\n",
    "def mmcut(text: str) -> List[str]:\n",
    "    res = []\n",
    "    for w in _multicut(text):\n",
    "        mm = min(w.multi, key=lambda x: x.count(\"/\"))\n",
    "        res.extend(mm.split(\"/\"))\n",
    "    return res\n",
    "\n",
    "\n",
    "def _combine(ww: List[LatticeString]) -> Iterator[str]:\n",
    "    if ww == []:\n",
    "        yield \"\"\n",
    "    else:\n",
    "        w = ww[0]\n",
    "        for tail in _combine(ww[1:]):\n",
    "            if w.unique:\n",
    "                yield w + \"|\" + tail\n",
    "            else:\n",
    "                for m in w.multi:\n",
    "                    yield m.replace(\"/\", \"|\") + \"|\" + tail\n",
    "\n",
    "\n",
    "def segment(\n",
    "    text: str, custom_dict: Trie = DEFAULT_WORD_DICT_TRIE\n",
    ") -> List[str]:\n",
    "    \"\"\"Dictionary-based maximum matching word segmentation.\n",
    "\n",
    "    :param text: text to be tokenized\n",
    "    :type text: str\n",
    "    :param custom_dict: tokenization dictionary,\\\n",
    "        defaults to DEFAULT_WORD_DICT_TRIE\n",
    "    :type custom_dict: Trie, optional\n",
    "    :return: list of segmented tokens\n",
    "    :rtype: List[str]\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    return list(_multicut(text, custom_dict=custom_dict))\n",
    "\n",
    "\n",
    "def find_all_segment(\n",
    "    text: str, custom_dict: Trie = DEFAULT_WORD_DICT_TRIE\n",
    ") -> List[str]:\n",
    "    \"\"\"Get all possible segment variations.\n",
    "\n",
    "    :param text: input string to be tokenized\n",
    "    :type text: str\n",
    "    :param custom_dict: tokenization dictionary,\\\n",
    "        defaults to DEFAULT_WORD_DICT_TRIE\n",
    "    :type custom_dict: Trie, optional\n",
    "    :return: list of segment variations\n",
    "    :rtype: List[str]\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    ww = list(_multicut(text, custom_dict=custom_dict))\n",
    "\n",
    "    return list(_combine(ww))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_segment = \"\"\"\n",
    "\n",
    "ជាការរចនាមួយដែលអាចឲ្យយើងពាក់ជាប់ជាមួយនឹងត្រចៀកដោយមិនមានការឈឺ ទទួលបានគុណភាពសំឡេងល្អ លើសពីនេះទៅទៀតពេលដែលពាក់កាស់នេះជាប់ត្រចៀក យើងនៅតែអាចឮអ្នកជុំវិញខ្លួននិយាយឬក៏ធ្វើការសន្ទនាជាមួយគ្នាយ៉ាងល្អ ហើយវាក៏ជួយរក្សាការសម្ងាត់នូវរាល់អ្វីដែលយើងបានស្តាប់មិនឲ្យឮចេញចេញក្រៅផងដែរ។ \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment(text_all_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all_segment(text_all_segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Generic functions of tokenizers\n",
    "\"\"\"\n",
    "import re\n",
    "from typing import Iterable, List, Union\n",
    "\n",
    "\n",
    "DEFAULT_WORD_TOKENIZE_ENGINE = \"multi_cut\"\n",
    "\n",
    "\n",
    "from _utils import (\n",
    "    apply_postprocessors,\n",
    "    rejoin_formatted_num,\n",
    "    strip_whitespace,\n",
    ")\n",
    "# from khmernlp.util.trie import Trie, dict_trie\n",
    "\n",
    "\n",
    "def word_tokenize(\n",
    "    text: str,\n",
    "    custom_dict: Trie = Trie([]),\n",
    "    engine: str = DEFAULT_WORD_TOKENIZE_ENGINE,\n",
    "    keep_whitespace: bool = True,\n",
    "    join_broken_num: bool = True,\n",
    ") -> List[str]:\n",
    "\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    segments = []\n",
    "\n",
    "\n",
    "    if engine in (\"mm\", \"multi_cut\"):\n",
    "        # from pythainlp.tokenize.multi_cut import segment\n",
    "        # import segment\n",
    "        segments = segment(text, custom_dict)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"\"\"Tokenizer \\\"{engine}\\\" not found.\n",
    "            It might be a typo; if not, please consult our document.\"\"\"\n",
    "        )\n",
    "\n",
    "    postprocessors = []\n",
    "    if join_broken_num:\n",
    "        postprocessors.append(rejoin_formatted_num)\n",
    "\n",
    "    if not keep_whitespace:\n",
    "        postprocessors.append(strip_whitespace)\n",
    "\n",
    "    segments = apply_postprocessors(segments, postprocessors)\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        custom_dict: Union[Trie, Iterable[str], str] = [],\n",
    "        engine: str = \"multi_cut\",\n",
    "        keep_whitespace: bool = True,\n",
    "        join_broken_num: bool = True,\n",
    "    ):\n",
    "\n",
    "        self.__trie_dict = Trie([])\n",
    "        if custom_dict:\n",
    "            self.__trie_dict = dict_trie(custom_dict)\n",
    "        else:\n",
    "            self.__trie_dict = DEFAULT_WORD_DICT_TRIE\n",
    "        self.__engine = engine\n",
    "        if self.__engine not in [\"newmm\", \"mm\", \"longest\", \"deepcut\", \"multi_cut\"]:\n",
    "            raise NotImplementedError(\n",
    "                \"\"\"\n",
    "                The Tokenizer class is not support %s for custom tokenizer\n",
    "                \"\"\"\n",
    "                % self.__engine\n",
    "            )\n",
    "        self.__keep_whitespace = keep_whitespace\n",
    "        self.__join_broken_num = join_broken_num\n",
    "\n",
    "    def word_tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Main tokenization function.\n",
    "\n",
    "        :param str text: text to be tokenized\n",
    "        :return: list of words, tokenized from the text\n",
    "        :rtype: list[str]\n",
    "        \"\"\"\n",
    "        return word_tokenize(\n",
    "            text,\n",
    "            custom_dict=self.__trie_dict,\n",
    "            engine=self.__engine,\n",
    "            keep_whitespace=self.__keep_whitespace,\n",
    "            join_broken_num=self.__join_broken_num,\n",
    "        )\n",
    "\n",
    "    def set_tokenize_engine(self, engine: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the tokenizer's engine.\n",
    "\n",
    "        :param str engine: choose between different options of tokenizer engines\n",
    "                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)\n",
    "        \"\"\"\n",
    "        self.__engine = engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Khmercut with multicut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "\n",
    "បើតាមសេចក្តីប្រកាស​ព័ត៌មានមួយ​របស់​ក្រសួងការបរទេស​កម្ពុជា លោកវ៉ាង យី នឹងចូល​ក្រាប​បង្គំ​គាល់​ព្រះមហាក្សត្រ​ព្រះបាទ​សម្តេច​នរោត្តម សីហមុនី​ នៅ​ព្រះបរម​រាជវាំង។\n",
    "\n",
    "ប្រមុខការទូត​ចិនរូបនេះ​ក៏គ្រោង​ចូល​ជួប​សម្តែង​ការ​គួរសម​ដាច់ដោយ​ឡែក​ជាមួយប្រធាន​ព្រឹទ្ធ​សភា លោក ហ៊ុន​ សែន និង​នាយករដ្ឋមន្ត្រី​លោក ហ៊ុន ម៉ាណែត។ ក្រៅ​ពីជំនួបនេះ​ លោក​វ៉ាង យី​ ក៏​នឹង​ជួបពិភាក្សា​ជាមួយសម​ភាគីរបស់​លោក​គឺ​រដ្ឋមន្ត្រី​ការបរទេស​និង​កិច្ចសហប្រតិបត្តិការ​អន្ត​រជាតិ​ លោក​សុខ ចិន្តា​សោភា​។\n",
    "\n",
    "អំឡុងពេល​មាន​វត្តមាន​នៅប្រទេស​កម្ពុជា លោក វ៉ាង យី​ នឹង​ធ្វើជាសហប្រធាន​ជាមួយ​ឧប​នាយក​រដ្ឋមន្ត្រី​ និង​ជា​អនុប្រធានទី​១ ក្រុមប្រឹក្សា​អភិវឌ្ឍន៍កម្ពុជា ដឹកនាំ​កិច្ចប្រជុំ​គណៈកម្មាធិការ​សម្រប​សម្រួល​អន្តររដ្ឋាភិបាល​កម្ពុជា-ចិន​លើក​ទី​៧។ នេះបើយោងតាម​ក្រសួងការបរទេសកម្ពុជា។​\n",
    "\n",
    "ដំណើរ​ទស្សនកិច្ច​របស់​លោក វ៉ាង យីមក​កាន់​ប្រទេស​កម្ពុជា​នៅពេល​នេះ ត្រូវអ្នកវិភាគ​មួយ​ចំនួន​មើល​ឃើញថា​ ជាការពង្រឹង​ទំនាក់​ទំនងរវាង​រដ្ឋាភិបាល​ថ្មីនៃប្រទេស​កម្ពុជា និង​រដ្ឋាភិបាល​ក្រុង​ប៉េកាំង ហើយ​ក៏ជា​សាទរ​ចំពោះអតីត​នាយករដ្ឋមន្ត្រីលោក​ហ៊ុន សែន​ដែល​ទើបតែ​បានកា្លយ​ជា​ប្រធាន​ព្រឹទ្ធសភា​នាពេល​ថ្មីៗនេះ​។\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "បើតាម\n",
      "សេចក្តី\n",
      "ប្រកាស\n",
      "ព័ត៌មាន\n",
      "មួយ\n",
      "របស់\n",
      "ក្រសួង\n",
      "ការបរទេស\n",
      "កម្ពុជា\n",
      "លោក\n",
      "វ៉ាង\n",
      "យី\n",
      "នឹង\n",
      "ចូល\n",
      "ក្រាប\n",
      "បង្គំ\n",
      "គាល់\n",
      "ព្រះមហាក្សត្រ\n",
      "ព្រះបាទ\n",
      "សម្តេច\n",
      "នរោត្តម\n",
      "សីហមុនី\n",
      "នៅ\n",
      "ព្រះ\n",
      "បរម\n",
      "រាជវាំង\n",
      "។\n",
      "ប្រមុខ\n",
      "ការទូត\n",
      "ចិន\n",
      "រូបនេះ\n",
      "ក៏\n",
      "គ្រោង\n",
      "ចូល\n",
      "ជួប\n",
      "សម\n",
      "្\n",
      "តែង\n",
      "ការ\n",
      "គួរសម\n",
      "ដាច់\n",
      "ដោយ\n",
      "ឡែក\n",
      "ជាមួយ\n",
      "ប្រធាន\n",
      "ព្រឹទ្ធ\n",
      "សភា\n",
      "លោក\n",
      "ហ៊ុន\n",
      "សែន\n",
      "និង\n",
      "នាយករដ្ឋមន្ត្រី\n",
      "លោក\n",
      "ហ៊ុន\n",
      "ម៉ាណែត\n",
      "។\n",
      "ក្រៅ\n",
      "ពី\n",
      "ជំនួប\n",
      "នេះ\n",
      "លោក\n",
      "វ៉ាង\n",
      "យី\n",
      "ក៏\n",
      "នឹង\n",
      "ជួប\n",
      "ពិភាក្សា\n",
      "ជាមួយសម\n",
      "ភាគី\n",
      "របស់\n",
      "លោក\n",
      "គឺ\n",
      "រដ្ឋមន្ត្រី\n",
      "ការបរទេស\n",
      "និង\n",
      "កិច្ច\n",
      "សហប្រតិបត្តិការ\n",
      "អន្ត\n",
      "រ\n",
      "ជាតិ\n",
      "លោក\n",
      "សុខ\n",
      "ចិន្តា\n",
      "សោភា\n",
      "​។\n",
      "អំឡុងពេល\n",
      "មាន\n",
      "វត្តមាន\n",
      "នៅ\n",
      "ប្រទេស\n",
      "កម្ពុជា\n",
      "លោក\n",
      "វ៉ាង\n",
      "យី\n",
      "នឹង\n",
      "ធ្វើជាសហប្រធាន\n",
      "ជាមួយ\n",
      "ឧ\n",
      "ប\n",
      "នាយក\n",
      "រដ្ឋមន្ត្រី\n",
      "និង\n",
      "ជា\n",
      "អនុប្រធាន\n",
      "ទី\n",
      "១\n",
      "ក្រុមប្រឹក្សា\n",
      "អភិវឌ្ឍន៍\n",
      "កម្ពុជា\n",
      "ដឹកនាំ\n",
      "កិច្ចប្រជុំ\n",
      "គណៈកម្មាធិការ\n",
      "សម្រប\n",
      "សម្រួល\n",
      "អន្តររដ្ឋាភិបាល\n",
      "កម្ពុជា\n",
      "-\n",
      "ចិន\n",
      "លើក\n",
      "ទី\n",
      "៧\n",
      "។\n",
      "នេះ\n",
      "បើយោងតាម\n",
      "ក្រសួង\n",
      "ការបរទេសកម្ពុជា\n",
      "។​\n",
      "ដំណើរ\n",
      "ទស្សនកិច្ច\n",
      "របស់\n",
      "លោក\n",
      "វ៉ាង\n",
      "យីមក\n",
      "កាន់\n",
      "ប្រទេស\n",
      "កម្ពុជា\n",
      "នៅពេល\n",
      "នេះ\n",
      "ត្រូវ\n",
      "អ្នក\n",
      "វិភាគ\n",
      "មួយ\n",
      "ចំនួន\n",
      "មើល\n",
      "ឃើញថា\n",
      "ជា\n",
      "ការ\n",
      "ពង្រឹង\n",
      "ទំនាក់\n",
      "ទំនងរវាង\n",
      "រដ្ឋាភិបាល\n",
      "ថ្មី\n",
      "នៃ\n",
      "ប្រទេស\n",
      "កម្ពុជា\n",
      "និង\n",
      "រដ្ឋាភិបាល\n",
      "ក្រុង\n",
      "ប៉េកាំង\n",
      "ហើយ\n",
      "ក៏\n",
      "ជា\n",
      "សាទរ\n",
      "ចំពោះ\n",
      "អតីត\n",
      "នាយករដ្ឋមន្ត្រី\n",
      "លោក\n",
      "ហ៊ុន\n",
      "សែន\n",
      "ដែល\n",
      "ទើបតែ\n",
      "បានកា\n",
      "្\n",
      "លយ\n",
      "ជា\n",
      "ប្រធាន\n",
      "ព្រឹទ្ធសភា\n",
      "នាពេល\n",
      "ថ្មីៗ\n",
      "នេះ\n",
      "​។\n"
     ]
    }
   ],
   "source": [
    "# segment(text)\n",
    "\n",
    "ignore_words = [\n",
    "    '\\n', \n",
    "    ' ', \n",
    "    '  ', \n",
    "    '\\u200b', \n",
    "    '\\t', \n",
    "    '\\r'\n",
    "]\n",
    "\n",
    "correct_words = {\n",
    "    \"ពិេ សស\": \"ពិេសស\",\n",
    "    \"ពីេសស\": \"ពិេសស\",\n",
    "    \"ខ្បាល\": \"ក្បាល\",\n",
    "    \"កម្ពុ\": \"កម្ពុជា\",\n",
    "}\n",
    "\n",
    "custom_words = [\n",
    "    \"ខ្បាល\",\n",
    "    \"គណអធីបតី\",\n",
    "    \"១០\",\n",
    "    \"ណែរនាំ\",\n",
    "    \"P60\",\n",
    "    \"កម្ពុជា\", # still error on word \"កម្ពុជា\"\n",
    "    \"ថេប្លេត\",\n",
    "    \"Galaxy\",\n",
    "    \"S23\",\n",
    "    \"Ultra\"\n",
    "    \"S9+\",\n",
    "    \"S9\",\n",
    "    \"សំខាន់ៗ​\",\n",
    "    \"​មជ្ឈិម​បក្ស​កុម្មុយ​និស្ត\",\n",
    "    \"កុម្មុយ​និស្ត\",\n",
    "    \"សេចក្តី\",\n",
    "    \"អន្តរជាតិ\",\n",
    "    \"បានក្លាយជា\"\n",
    "    \n",
    "]\n",
    "\n",
    "custome_words_list = set(khmer_words())\n",
    "# add multiple words to the dictionary\n",
    "custome_words_list.update(custom_words)\n",
    "\n",
    "trie = dict_trie(dict_source=custome_words_list)\n",
    "\n",
    "custom_tokenizer = Tokenizer(custom_dict=trie, engine=\"multi_cut\")\n",
    "\n",
    "list_results = custom_tokenizer.word_tokenize(text)\n",
    "\n",
    "\n",
    "for word in list_results:\n",
    "    if word not in ignore_words:\n",
    "        if word in correct_words:\n",
    "            print(correct_words[word])\n",
    "        else:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### khmercut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ព័ត៌មាន\n",
      "នៅ\n",
      "ថ្ងៃ\n",
      "នេះ\n",
      "មាន\n",
      "ដូចជា\n",
      "បទសម្ភាសន៍\n",
      "VOA\n",
      "អំពី\n",
      "​ជន\n",
      "​រងគ្រោះ\n",
      "នៃ\n",
      "​របប\n",
      "ប្រល័យ\n",
      "ពូជសាសន៍\n",
      "ចែក\n",
      "រំលែក\n",
      "បទពិសោធន៍\n",
      "​តស៊ូ\n",
      "នៅ\n",
      "អាមេរិក\n",
      "និង\n",
      "អតីតកាល\n",
      "។\n",
      "បទសម្ភាសន៍\n",
      "វីអូអេ\n",
      "ជាមួយ\n",
      "នឹង\n",
      "លោក\n",
      "វេជ្ជបណ្ឌិត\n",
      "សៀង\n",
      "សេង\n",
      "នៅ\n",
      "រដ្ឋ\n",
      "California\n",
      "អំពី\n",
      "​«\n",
      "ជំងឺ\n",
      "លើស\n",
      "ឈាម\n",
      "»\n",
      "និង\n",
      "ព័ត៌មាន\n",
      "ផ្សេង\n",
      "ទៀត\n",
      "៕\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from khmercut import tokenize\n",
    "\n",
    "# tokenize(text)\n",
    "\n",
    "ignore_words = [\n",
    "    '\\n',\n",
    "    '\\n\\n',\n",
    "    '\\n\\n\\n',\n",
    "    '',\n",
    "    ' ', \n",
    "    '  ', \n",
    "    '\\u200b', \n",
    "    '\\t', \n",
    "    '\\r'\n",
    "]\n",
    "\n",
    "for word in tokenize(text):\n",
    "    if not word in ignore_words:\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khmerasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
